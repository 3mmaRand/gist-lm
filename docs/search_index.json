[
["index.html", "gist-lm: Get Introductory Statistical Tests as Linear models: A guide for R users Preface 0.1 Who is this book for? 0.2 Approach of this book 0.3 Options on the toolbar 0.4 Conventions used in the book 0.5 Following along with the examples 0.6 Overview of the chapter contents", " gist-lm: Get Introductory Statistical Tests as Linear models: A guide for R users Emma Rand August 2020 Preface 0.1 Who is this book for? This book designed to help people with a little experience of data analysis in R get the gist of linear models in R. It is aimed at non-specialists who have learned introductory data analysis as a skill for doing research in another field. The audience I had in mind when writing the book are undergraduates on degree programmes in the life sciences who have done an introductory course covering hypothesis testing with single linear regression, t-tests, and ANOVA. However, students on social science, media, finance and education degree programmes often do similar introductory data analysis courses. This book can also help them, although the examples are from the life sciences. A short revision chapter giving an overview of introductory data analysis course is included to set the scene and clarify terminology used in the rest of the book. The coverage of single linear regression in the first chapter of Part 2 is also likely to be revision for most readers. I assume you have an understanding of the rationale of hypothesis testing and some experience selecting, applying and interpreting tests. I also assume you have some familiarity with R and RStudio and have general, but not expert, proficiency in summarising, analysing and visualising data with functions such as t.test(), aov(), TukeyHSD() and ggplot(). I do not assume your fluency allows you to do these things without looking things up. The book has two aims. First to explain how the t-test, ANOVA and regression are actually all the same test and introduce the terminology of statistical modelling and, secondly, to teach you how to use and interpret the lm() function. 0.2 Approach of this book Regression, t-tests and one-way ANOVA are special cases of a much more widely applicable statistical model known as the “general linear model”. Since they are fundamentally the same test, all can be carried out with the lm() function in R. However, it is common for t-tests and ANOVA to be taught to non-specialists using the t.test() and aov() functions respectively. There are some sensible reasons for this. For example, many introductory texts take the same approach and typically, the outputs of t.test() and aov() are easier for beginners to understand and interpret. However, the output of lm() is more typical of statistical modelling functions in general and these are made harder to understand if you are not used to using lm() for the relatively simple cases. This makes the use of only slightly more advanced methods seem like a bigger leap in understanding than it really is, and extending your statistical repertoire more intimidating than it could be. The approach taken in this book is to exploit your pre-existing knowledge of t-tests and ANOVA using t.test() and aov() to understand the output of lm(). lm() can be used to perform t-tests, ANOVAs and regression. Examples are carried out with the familiar functions and then with lm() so you can make the link between the two. Each example demonstrates the R code needed, how understand the output and how to report the results, including suggested ggplot2 figures. The code is given for figures but, as this isn’t a book about ggplot2, it is not extensively explained. Readers keen to learn more about ggplot2 are advised to go to https://ggplot2.tidyverse.org/. 0.3 Options on the toolbar You can change the appearance of the book using the toolbar at the top of the page. The menu on the left can be hidden, the font size increased or decreased and the colour altered to a dark or sepia theme. Search the book by clicking on the magnifying glass, entering a search term and using the up and down arrows to navigate through the results. 0.4 Conventions used in the book Code and any output appears in blocks formatted like this: stag &lt;- read_table2(&quot;data-raw/stag.txt&quot;) glimpse(stag) # Rows: 16 # Columns: 2 # $ jh &lt;dbl&gt; 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140... # $ mand &lt;dbl&gt; 0.56, 0.35, 0.28, 1.22, 0.48, 0.86, 0.68, 0.77, 0.55, 1.18, 0.... Lines of output start with a #. Within the text: * packages are indicated in bold code font like this: ggplot2 * functions are indicated in code font with brackets after their name like this: ggplot() * R objects are indicated in code font like this: stag Key points are summarised throughout the book using boxes like this: The key point of a previous few paragraphs is in boxes like these Extra pieces of information that are not essential to understanding the material are presented like this: Extra information and tips are in boxes like these 0.5 Following along with the examples Readers may wish to code along and the following gives guidance on how best to do that. I recommend starting a new RStudio project and creating a folder inside that project called data-raw where you will save the data files. Links to the data files are given in the text and these can be downloaded to your data-raw folder by right-clicking the link choosing the option to save. Then make a new script file for each example to carry our the analysis for that example. For example, if you call your Project gist and you have just started Chapter 4 Single linear regression, your folder structure would look like this: -- gist |-- gist.Rproj |-- stagbeetle_regresion.R |-- data-raw |-- stag.text Using this structure will mean the paths to files needed in your code are the same as those given in the book. The content of a code block can be copied using the icon in its top right corner. I use packages from the tidyverse (Wickham et al. 2019) including ggplot2 (Wickham 2016), dplyr (Wickham et al. 2020), tidyr (Wickham 2020) and readr (Wickham, Hester, and Francois 2018) throughout the book. All the code assumes you have loaded the core tidyverse packages with: library(tidyverse) If you run examples and get an error like this: # Error in read_table2(&quot;data-raw/stag.txt&quot;) : # could not find function &quot;read_table2&quot; It is likely you need to load the tidyverse as shown above. All other packages will be loaded explicitly with library() statements where needed. 0.6 Overview of the chapter contents Part 1 Introduction provides a very brief overview of a typical introductory data analysis class using terminology that will used throughout the remaining chapters. If the concepts in this chapter are very unfamiliar, you may benefit from revising your previous work. In Part 2 we revise single linear regression which is likely where you have previously encountered the lm() function. We then work through t-test, one-way ANOVA and two-way ANOVA examples carried out first with t.test() and aov() and then with lm() to gain a good understanding of the lm() output and interrogation for reporting. I used the knitr package (Xie 2015) and the bookdown package (Xie 2020) to compile my book. My R session information is shown below: sessionInfo() # R version 4.0.2 (2020-06-22) # Platform: x86_64-w64-mingw32/x64 (64-bit) # Running under: Windows 10 x64 (build 16299) # # Matrix products: default # # locale: # [1] LC_COLLATE=English_United Kingdom.1252 # [2] LC_CTYPE=English_United Kingdom.1252 # [3] LC_MONETARY=English_United Kingdom.1252 # [4] LC_NUMERIC=C # [5] LC_TIME=English_United Kingdom.1252 # # attached base packages: # [1] stats graphics grDevices utils datasets methods base # # other attached packages: # [1] patchwork_1.0.1 kableExtra_1.2.1 forcats_0.5.0 stringr_1.4.0 # [5] dplyr_1.0.2 purrr_0.3.4 readr_1.3.1 tidyr_1.1.2 # [9] tibble_3.0.3 ggplot2_3.3.2 tidyverse_1.3.0 # # loaded via a namespace (and not attached): # [1] tidyselect_1.1.0 xfun_0.16 haven_2.3.1 colorspace_1.4-1 # [5] vctrs_0.3.4 generics_0.0.2 htmltools_0.5.0 viridisLite_0.3.0 # [9] yaml_2.2.1 utf8_1.1.4 blob_1.2.1 rlang_0.4.7 # [13] pillar_1.4.6 glue_1.4.1 withr_2.2.0 DBI_1.1.0 # [17] dbplyr_1.4.4 modelr_0.1.8 readxl_1.3.1 lifecycle_0.2.0 # [21] munsell_0.5.0 gtable_0.3.0 cellranger_1.1.0 rvest_0.3.6 # [25] evaluate_0.14 knitr_1.29 fansi_0.4.1 broom_0.7.0 # [29] Rcpp_1.0.5 scales_1.1.1 backports_1.1.9 webshot_0.5.2 # [33] jsonlite_1.7.0 fs_1.5.0 hms_0.5.3 digest_0.6.25 # [37] stringi_1.4.6 bookdown_0.20 grid_4.0.2 cli_2.0.2 # [41] tools_4.0.2 magrittr_1.5 crayon_1.3.4 pkgconfig_2.0.3 # [45] ellipsis_0.3.1 xml2_1.3.2 reprex_0.3.0 lubridate_1.7.9 # [49] assertthat_0.2.1 rmarkdown_2.3 httr_1.4.2 rstudioapi_0.11 # [53] R6_2.4.1 compiler_4.0.2 References "],
["revision.html", "Chapter 1 Revision of your Introductory class", " Chapter 1 Revision of your Introductory class In experimental design and execution we manipulate, or choose, one or more variables and record the effect of this manipulation on another variable. The variables we manipulate are called explanatory or predictor variables and the other is called the response. These are also known as independent and dependent variables respectively. Predictor, Explanatory, x and Independent: all terms used to describe the variables we choose. Predicted, Response, y and Dependent: all terms used to describe the variable we measure. When we plot data, the response variable goes on the y-axis and the explanatory variable goes on the x-axis. If we have two explanatory variables we need another way to visualise it. Often we might indicate the different values of a second explanatory variable with colour. The third explanatory variable can be displayed using different panels. See Figure 1.1. Figure 1.1: Explanatory variables are placed on the x-axis and, if there is more than one, indicated with different colours (or shapes) and panels. The response variable is always on the y-axis. In choosing between regression, t-tests, one-way ANOVA and two-way ANOVA we consider how many explanatory variables we have and whether they are continuous or categorical. If we have one continuous explanatory variable we can apply a regression. If the explanatory variable is categorical with two groups (or levels) we have the choice of a t-test or a one-way ANOVA, but when there are more than two groups we must use a one-way ANOVA. A two-way ANOVA is used when there are two categorical explanatory variables. See Figure 1.2. Figure 1.2: Decision tree for choosing between single linear regression, t-tests, one-way ANOVA and two-way ANOVA. These apparently different tests are, in fact, the same test. They have the same underlying mathematics, or to put it another way, they all follow the same model. That model is often known as the General Linear Model. "],
["what-are-linear-models.html", "Chapter 2 What are General Linear Models 2.1 Overview 2.2 Model fitting 2.3 More than one explanatory variable 2.4 General linear models in R 2.5 Reporting", " Chapter 2 What are General Linear Models 2.1 Overview A general linear model describes a continuous response variable as a function of one or more explanatory variables. For a single explanatory variable, the model is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i} \\tag{2.1} \\end{equation}\\] Where: \\(y\\) is the response variable and \\(X1\\) is the explanatory variable. \\(i\\) is the index of the values so \\(X1_{i}\\) is the \\(i\\)th value of \\(X1\\) \\(E(y_{i})\\) is the expected value of \\(y\\) for the \\(i\\)th value of \\(X1\\). \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are the coefficients - or parameters - in the model. In a single linear regression, \\(\\beta_{0}\\) is often called the intercept and \\(\\beta_{1}\\) the slope. The equation (2.1) allows the response, \\(y\\), to be predicted for a given value of the explanatory variable. Let’s unpack what we mean by \\(E(y_{i})\\), the expected value of \\(y\\). If you measure the response for a particular value of \\(x\\) very many times there would be some distribution of those responses. In the general linear model that distribution is assumed to be normal and its mean is the expected value for \\(y\\), \\(E(y_{i})\\). Another way of saying that is that in a general linear model, we “model the mean of the response”. That the measured response is drawn from normal distribution with a mean of \\(E(y_{i})\\) is a defining feature of the general linear model. An additional assumption is that those normal distributions have the same variance for all the \\(x\\) values. 2.2 Model fitting The process of estimating the model coefficients from your data (set of chosen \\(X1\\) with their measured \\(y\\) values) is known as fitting a linear model. The coefficients are also known as parameters. The measured response values in your data, \\(y_{i}\\), will differ from the predicted values, \\(\\hat{y}\\), randomly and these random differences are known as residuals or errors. Our parameter values are chosen to minimise the sum of the squared residuals. A very commonly used abbreviation for the sum of the squared residuals (or errors) is \\(SSE\\). \\[\\begin{equation} SSE = \\sum(y_{i}-\\hat{y})^2 \\tag{2.2} \\end{equation}\\] Since the coefficient values are those that minimise the \\(SSE\\), those values are known as least squares estimates. The mean of a sample is also a least squares estimate - The role played by \\(SSE\\) in estimating our parameters means that it is also used in determining how well our model fits our data. Our model can be considered useful if its predictions are close to the observed data and the smaller the value of \\(SSE\\), the better the fit. In other words, there is little random variance left over in the response. The absolute value of \\(SSE\\) will depend on the size of the \\(y\\) values and the sample size so it would be difficult to compare two models. Instead, we express it as a proportion of the total variation in \\(y\\), \\(SST\\): \\[\\begin{equation} SSE / SST \\tag{2.3} \\end{equation}\\] This is the proportion of variance left over after the model fitting. The proportion of variance explained by the model is a very commonly used metric of model fit and you have probably heard of it - R-squared, \\(R^2\\). It is: \\[\\begin{equation} R^2=1-\\frac{SSE}{SST} \\tag{2.3} \\end{equation}\\] If there were no explanatory variables, the value we would predict for the response variable is its mean. Thus a good model should fit the response better than the mean. The output of lm() includes the \\(R^2\\). It represents the proportional improvement in the predictions from the regression model relative to the mean model. It ranges from zero, the model is no better than the mean, to 1, the predictions are perfect. See Figure 2.1. Figure 2.1: A linear model with different fits. A) the model is a poor fit - the explanatory variable is no better than the response mean for predicting the response. B) the model is good fit - the explanatory variable explains a high proportion of the variance in the response. C) the model is a perfect fit - the response can be predicted perfectly from the explanatory variable. Measured response values are in pink, the predictions are in green and the dashed blue line gives the mean of the response. Since the distribution of the responses for a given \\(x\\) is assumed to be normal and the variances of those distributions are assumed to be homogeneous, both are also true of the residuals. It is our examination of the residuals which allows us to evaluate whether the assumptions are met. See Figure 2.2 for a graphical representation of linear modelling terms introduced so far. We will reference this figure in later chapters. Figure 2.2: A general linear model annotated with the terms used in modelling. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters, \\(\\beta_{0}\\) (the intercept) and \\(\\beta_{1}\\) (the slope) are indicated. 2.3 More than one explanatory variable When you have more than one explanatory variable these are given as \\(X2\\), \\(X3\\) and so on up to the \\(p\\)th explanatory variable. Each explanatory variable has its own \\(\\beta\\) coefficient. The general form of the model is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i}+...+\\beta_{p}Xp_{i} \\tag{2.4} \\end{equation}\\] The model has only one intercept which is the value of the response when all the explanatory variables are zero. There is one problem with \\(R^2\\) as a measure of model fit: the more explanatory variables that are added, the higher the \\(R^2\\). This is true even if the added variables explain a really tiny amount of the variance in the response. Using \\(R^2\\) to select a model would mean always choosing the model with the most variables in it. However, a key aim of statistical modelling is to understand the response and this is easier for simpler models. We want to find a balance between the complexity of the model and its explanatory power. The Adjusted \\(R^2\\) is a way to achieve this. It reduces the \\(R^2\\) for each coefficient added. A related reason for not including variables that explain only tiny amounts of variance is overfitting. Overfitting occurs when your model fits your data very well but does not generalise. We want models that would predict the response equally well for a new set of data. 2.4 General linear models in R 2.4.1 Building and viewing T-tests and ANOVA, like regression, can be carried out with the lm() function in R. It uses the same method for specifying the model. When you have one explanatory variable the command is: lm(data = dataframe, response ~ explanatory) The response ~ explanatory part is known as the model formula. When you have two explanatory variable we add the second explanatory variable to the formula using a + or a *. The command is: lm(data = dataframe, response ~ explanatory1 + explanatory2) or lm(data = dataframe, response ~ explanatory1 * explanatory2) A model with explanatory1 + explanatory2 considers the effects of the two variables independently. A model with explanatory1 * explanatory2 considers the effects of the two variables and any interaction between them. We usually assign the output of lm() commands to an object and view it with summary(). The typical workflow would be: mod &lt;- lm(data = dataframe, response ~ explanatory) summary(mod) There are two sorts of statistical tests in the output of summary(mod): tests of whether each coefficient is significantly different from zero; and an F-test of the model overall. The F-test in the last line of the output indicates whether the relationship modelled between the response and the set of explanatory variables is statistically significant. lm() can be used to perform tests using the General Linear Model including t-tests, ANOVA and regression for response variables which are normally distributed. Elements of the lm() object include the estimated coefficients, the predicted values and the residuals. These can be accessed with mod$coeffients, mod$fitted.values and mod$residuals respectively. 2.4.2 Getting predictions mod$fitted.values gives the predicted values for the explanatory variable values actually used in the experiment, i.e., there is a prediction for each row of data. To get predictions for a different set of values we need to make a dataframe of the different set of values and use the predict() function. The typical workflow would be: predict_for &lt;- data.frame(explanatory = values) predict_for$pred &lt;- predict(mod, newdata = predict_for) 2.4.3 Checking assumptions The assumptions of the model are checked using the plot() function which produces diagnostic plots to explore the distribution of the residuals. They are not proof of the assumptions being met but allow us to quickly determine if the assumptions are plausible, and if not, how the assumptions are violated and what data points contribute to the violation. The two plots which are most useful are the “Q-Q” plot (plot 2) and the “Residuals vs Fitted” plot (plot 1). These are given as values to the which argument of plot(). 2.4.3.1 The Q-Q plot The Q-Q plot is a scatterplot of the residuals (standardised to a mean of zero and a standard deviation of 1) against what is expected if the residuals are normally distributed. plot(mod, which = 2) The points should fall roughly on the line if the residuals are normally distributed. In the example above, the residuals appear normally distributed. The following are two examples in which the residuals are not normally distributed. If you see patterns like these you should find an alternative to a general linear model such as a non-parametric test or a generalised linear model. Sometimes, applying a transformation to the response variable will result in better meeting the assumptions. 2.4.3.2 The Residuals vs Fitted plot The Residuals vs Fitted plot shows if residuals have homogeneous variance or non-linear patterns. Non-linear relationships between explanatory variables and the response will usually show in this plot if the model does not capture the non-linear relationship. For the assumptions to be met, the residuals should be equally spread around a horizontal line as they are here: plot(mod, which = 1) The following are two examples in which the residuals do not have homogeneous variance and display non-linear patterns. 2.5 Reporting to add figure: data + model summarise but in a way that is ‘honest’ direction and magnitude of effects, significance, stats result. "],
["terminolgy.html", "Chapter 3 Terminolgy 3.1 General? Generalised?", " Chapter 3 Terminolgy 3.1 General? Generalised? Before we get started, I need to explain some potentially confusing terminology. I use the term “general linear model” to mean classical linear regression models of a continuous response variable explained by one or more continuous or categorical variables. These include regression, t-tests, analysis of variance (ANOVA) and analysis of covariance (ANCOVA). I use the term “generalised linear model” or GLM to refer to a larger class of models formulated by John Nelder and Robert Wedderburn (1972) and popularised in “Generalized Linear Models”, an influential book by Peter McCullagh and John Nelder (1989). In these models, the response variable can follow a number of other distributions including Poisson and binomial distributions. In some texts, the general linear model is referred to by the acronym GLM and the generalised linear model as GLIM but in this book I use GLM to refer to generalised linear models only. References "],
["single-regression.html", "Chapter 4 Single linear regression 4.1 Introduction to the example 4.2 Applying and interpreting lm() 4.3 Link to Chapter 2.1 4.4 Getting predictions from the model 4.5 Checking assumptions 4.6 Creating a figure 4.7 Reporting the results", " Chapter 4 Single linear regression This is probably the one general linear model you have applied using lm() previously and it is covered here as revision and make more clear links between regression, t-tests and ANOVA. 4.1 Introduction to the example Figure 4.1: Male stag beetles Lucanus cervus, have large mandibles that resemble the antlers of a stag and give them their common and scientific name (Cervus is a genus of deer). By Simon A. Eugster - Own work, CC BY 3.0, https://commons.wikimedia.org/w/index.php?curid=7790887 The concentration of Juvenile growth hormone in male stag beetles (Lucanus cervus) is known to influence mandible growth. See Figure 4.1 Groups of ten stag beetles were treated with different concentrations of Juvenile growth hormone (pg\\(\\mu\\)l-1) and their average mandible size (mm) determined. The data are in stag.txt. Juvenile hormone is has been set by the experimenter and we would expect mandible size to be normally distributed. jh mand 0 0.56 10 0.35 20 0.28 30 1.22 40 0.48 50 0.86 60 0.68 70 0.77 80 0.55 90 1.18 100 0.71 110 1.44 120 1.32 130 1.66 140 1.23 150 1.17 There are 2 variables: jh, the concentration of Juvenile growth hormone and mand, the average mandible size (mm) of 10 stag beetles We will import the data with the read_table2() function from the readr package and plot it with ggplot() from the ggplot2 package. Both packages are part of the tidyverse. Import the data: stag &lt;- read_table2(&quot;data-raw/stag.txt&quot;) Visualising our data before any further analysis is sensible. In this case, it will help us determine if any relationship between the two variables is linear. A simple scatter plot is appropriate. ggplot(data = stag, aes(x = jh, y = mand)) + geom_point() The relationship between the two variables looks roughly linear. So far, common sense suggests the assumptions of regression are met. 4.2 Applying and interpreting lm() The lm() function is used to build the regression model: mod &lt;- lm(data = stag, mand ~ jh) This can be read as: fit a linear of model of mandible size explained by the concentration of Juvenile growth hormone. Printing mod to the console will reveal the estimated model parameters (coefficients) but little else: mod # # Call: # lm(formula = mand ~ jh, data = stag) # # Coefficients: # (Intercept) jh # 0.41934 0.00646 \\(\\beta_{0}\\) is labelled (Intercept) and \\(\\beta_{1}\\) is labelled jh. Thus, the equation of the line is: \\(mand\\) = 0.419 + 0.006\\(jh\\) More information, including statistical tests of the model and its parameters, is obtained by using summary(): summary(mod) # # Call: # lm(formula = mand ~ jh, data = stag) # # Residuals: # Min 1Q Median 3Q Max # -0.3860 -0.2028 -0.0975 0.1503 0.6069 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 0.41934 0.13943 3.01 0.0094 ** # jh 0.00646 0.00158 4.08 0.0011 ** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 0.292 on 14 degrees of freedom # Multiple R-squared: 0.543, Adjusted R-squared: 0.51 # F-statistic: 16.6 on 1 and 14 DF, p-value: 0.00113 The Coefficients table gives the estimated \\(\\beta_{0}\\) and \\(\\beta_{1}\\) again but along with their standard errors and tests of whether the estimates differ from zero. The estimated value for the intercept is 0.419 \\(\\pm\\) 0.139 and this differs significantly from zero (\\(p\\) = 0.009). The estimated value for the slope is 0.006 \\(\\pm\\) 0.002, also differs significantly from zero (\\(p\\) = 0.001). The three lines at the bottom of the output give information about the fit of the model to the data. The Multiple R-squared gives the proportion of the variance in the response which is explained by the model. In our case, 0.543 of the variance in mandible length is explained by the model and this is a significant proportion of that variance (\\(p\\) = 0.001). The p-value for the model and the p-value for the slope are the same in a single linear regression because, except for the intercept, there is only one parameter (the slope) in the model. Linear models in the form of a two-sample t-test also estimate just one parameter and its p-value will also equal the model p-value. This is not the case for other linear models. 4.3 Link to Chapter 2.1 The estimated coefficients tell us mandible size is predicted to be 0.419 when Juvenile growth hormone is zero and increases by 0.006 mm for each pg\\(\\mu\\)l-1 of Juvenile growth hormone. At a Juvenile growth hormone of 1 pg\\(\\mu\\)l-1 the mandible is predicted to be 0.419 + 0.006 = 0.426 mm. At 2 pg\\(\\mu\\)l-1 the predicted mandible size is 0.419 + 0.006 + 0.006 = 0.432 mm. In general mandible size is: \\(\\beta_{0}\\) + \\(x\\times\\beta_{0}\\) mm at \\(x\\) pg\\(\\mu\\)l-1. See Figure 4.2 for a version of Figure 2.2 annotated with values from this example. Figure 4.2: The model annotated with values from the stag beetle example. The measured response values are in pink, the predictions are in green, and the residuals, are in blue. One example of a measured value, a predicted value and the residual is shown for a Juvenile hormone of 130 pg\\(\\mu\\)l-1. The estimated model parameters, \\(\\beta_{0}\\) and \\(\\beta_{1}\\) are indicated. Compare to Figure 2.2. 4.4 Getting predictions from the model The predict() function returns the predicted values of the response. To add a column of predicted values to the stag dataframe we use: stag$pred &lt;- predict(mod) glimpse(stag) # Rows: 16 # Columns: 3 # $ jh &lt;dbl&gt; 0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140... # $ mand &lt;dbl&gt; 0.56, 0.35, 0.28, 1.22, 0.48, 0.86, 0.68, 0.77, 0.55, 1.18, 0.... # $ pred &lt;dbl&gt; 0.419, 0.484, 0.549, 0.613, 0.678, 0.742, 0.807, 0.871, 0.936,... This gives predictions for the actual Juvenile growth hormone concentration values used. If you want predictions for other values, you need to create a data frame of the Juvenile growth hormone values from which you want to predict. The following creates a dataframe with one column of Juvenile growth hormone values from 0 to 150 in steps of 5: predict_for &lt;- data.frame(jh = seq(0, 150, 5)) glimpse(predict_for) # Rows: 31 # Columns: 1 # $ jh &lt;dbl&gt; 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80... Note that the column is named jh - the same as in the dataset and the model. Its variable type must also match. To predict responses for a new set of explanatory variable values, the name and type of explanatory variables in the new set must match those in the model. To get predicted mandible sizes for the Juvenile growth hormone values we use: predict_for$pred &lt;- predict(mod, newdata = predict_for) glimpse(predict_for) # Rows: 31 # Columns: 2 # $ jh &lt;dbl&gt; 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, ... # $ pred &lt;dbl&gt; 0.419, 0.452, 0.484, 0.516, 0.549, 0.581, 0.613, 0.645, 0.678,... 4.5 Checking assumptions The two assumptions of the model can be checked using diagnostic plots. The Q-Q plot is obtained with: plot(mod, which = 2) This sample is relatively small so we should expect more wiggliness than we saw in 2.2 but this looks OK. Let’s look at the Residuals vs Fitted plot: plot(mod, which = 1) Again the red line wiggles a little but there is no particular pattern and it appears that the variance is homogeneous along mandible size. 4.6 Creating a figure A suitable figure includes the data themselves and the model fitted: ggplot(data = stag, aes(x = jh, y = mand)) + geom_point() + scale_x_continuous(expand = c(0.01, 0), limits = c(0, 160), name = expression(paste(&quot;Juvenile growth hormone (pg&quot;, mu, l^-1, &quot;)&quot;))) + scale_y_continuous(expand = c(0, 0), limits = c(0, 2), name = &quot;Mandible length (mm)&quot;) + geom_smooth(method = lm, se = FALSE, colour = &quot;black&quot;) + theme_classic() 4.7 Reporting the results There was a significant positive relationship between the concentration of Juvenile growth hormone and mandible length (\\(\\beta_{1}\\pm s.e.\\): 0.006 \\(\\pm\\) 0.002; \\(p\\) = 0.001). See figure 4.3. Figure 4.3: Relationship between the concentration of Juvenile growth hormone and mandible length. "],
["t-tests-revisit.html", "Chapter 5 t-tests revisited 5.1 Introduction to the example 5.2 t.test() output reminder 5.3 t-tests as linear models 5.4 Applying and interpreting lm() 5.5 Getting predictions from the model 5.6 Checking assumptions 5.7 Creating a figure 5.8 Reporting the results", " Chapter 5 t-tests revisited In this chapter we look at an example with one categorical explanatory variable which has two groups (or levels). We first use the familiar t.test() then use its output to help us understand the output of lm(). We will also make predictions from the model and report on our results. 5.1 Introduction to the example Some plant biotechnologists developed a genetically modified line of Cannabis sativa to increase its omega 3 fatty acids content. They grew 50 wild type and fifty modified plants to maturity, collect the seeds and measure the amount of omega 3 fatty acids (in arbitrary units). The data are in csativa.txt. They want to know if the wild type and modified plants differ significantly in their omega 3 fatty acid content. omega plant 48.5 modif 43.6 modif 51.2 modif 56.4 modif 56.0 modif 58.7 modif 39.1 modif 48.8 modif 55.5 modif 44.6 modif 46.5 modif 41.5 modif 40.8 modif 45.1 modif 46.1 modif 39.4 modif 47.2 modif 48.0 modif 50.7 modif 48.2 modif 48.4 modif 48.1 modif 56.7 modif 49.6 modif 49.1 modif 47.4 modif 59.9 modif 54.3 modif 61.9 modif 46.7 modif 58.3 modif 41.9 modif 52.7 modif 54.5 modif 59.6 modif 49.6 modif 47.4 modif 53.4 modif 48.1 modif 53.8 modif 42.8 modif 45.8 modif 42.4 modif 48.2 modif 49.8 modif 50.1 modif 48.4 modif 61.0 modif 41.3 modif 46.3 modif 58.5 wild 55.5 wild 58.7 wild 67.7 wild 41.4 wild 48.0 wild 64.5 wild 52.2 wild 54.2 wild 40.5 wild 59.1 wild 68.5 wild 47.9 wild 60.5 wild 63.0 wild 57.5 wild 58.0 wild 70.2 wild 67.1 wild 52.7 wild 60.2 wild 42.5 wild 60.2 wild 53.8 wild 45.4 wild 53.2 wild 63.3 wild 45.3 wild 65.3 wild 61.9 wild 49.2 wild 73.3 wild 70.3 wild 56.3 wild 56.0 wild 53.5 wild 63.6 wild 45.9 wild 54.5 wild 54.6 wild 50.9 wild 58.2 wild 54.5 wild 56.6 wild 54.1 wild 53.5 wild 56.9 wild 46.0 wild 50.0 wild 65.7 wild There are 2 variables. plant is the explanatory variable; it is categorical with 2 levels, modif and wild. omega, a continuous variable, is the response. We again use the read_table2() function to import the data and visualise it with ggplot() csativa &lt;- read_table2(&quot;data-raw/csativa.txt&quot;) A quick plot of the data: ggplot(data = csativa, aes(x = plant, y = omega)) + geom_violin() Violin plots are a useful way to show the distribution of data in each group but not the only way. One alternative is geom_boxplot(). The modified plants have a lower mean omega 3 content than the wild type plants. The modification appears not to be successful! In fact, it may have significantly lowered the omega 3 content! Let’s create a summary of the data that will be useful for plotting later: csativa_summary &lt;- csativa %&gt;% group_by(plant) %&gt;% summarise(mean = mean(omega), std = sd(omega), n = length(omega), se = std/sqrt(n)) plant mean std n se modif 49.5 5.82 50 0.82 wild 56.4 7.85 50 1.11 Our summary confirms what we see in the plot. Statistical comparison of the two means can be done with either the t.test() or lm() functions; these are exactly equivalent but present the results differently. We will use our understanding of applying and interpreting t.test() to develop our understanding of lm() output 5.2 t.test() output reminder We can apply a two-sample t-test with: t.test(data = csativa, omega ~ plant, var.equal = TRUE) # # Two Sample t-test # # data: omega by plant # t = -5, df = 98, p-value = 2e-06 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # -9.69 -4.21 # sample estimates: # mean in group modif mean in group wild # 49.5 56.4 The two groups means are given in the section labelled sample estimates and the test of whether they differ significantly is given in the fourth line (beginning t = -5, df = ...). We conclude the mean omega 3 content of the modified plants (49.465 units) is significantly lower than that of the wild type plants (\\(t\\) = 5.029, \\(d.f.\\) = 98, \\(p\\) &lt; 0.001). The line under 95 percent confidence intervalgives the confidence limits on the difference between the two means. The sign on the \\(t\\) value and the confidence limits, and the order in which the sample estimates are given is determined by R’s alphabetical ordering of the groups. As “modif” comes before “wild” in the alphabet, “modif” is the first group. The statistical test is: the modified plant mean \\(-\\) the wild type mean. This ordering is arbitrary and has has no impact on our conclusions. If the wild type plants been labelled “control” so that “modif” would be the second group, our output would look like this: # Two Sample t-test # # data: omega by plant # t = 5.0289, df = 98, p-value = 2.231e-06 # alternative hypothesis: true difference in means is not equal to 0 # 95 percent confidence interval: # 4.205372 9.687828 # sample estimates: # mean in group control mean in group modif # 56.4118 49.4652 # Our conclusion would remain the same: 49.465 is significantly lower than 56.412. t.test() output: the estimates are the two group means and the p-value is for a test on the difference between them. 5.3 t-tests as linear models The equation for a t-test is just as it was for equation (2.1): \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i} \\tag{5.1} \\end{equation}\\] Remember, in a single linear regression \\(\\beta_{0}\\), the intercept, is the value of the response when the numerical explanatory variable is zero. So what does this mean when the explanatory variable is categorical? It means the intercept is the value of the response when the categorical explanatory is at its “lowest” level where the “lowest” level is the group which comes first alphabetically. \\(X1_{i}\\) is an indicator variable that takes the value of 0 or 1 and indicates whether the \\(i\\)th value was from one group or not. Such variables are known as dummy explanatory variables. They are dummy in the sense that they are numerical substitutes for the categorical variable whose ‘real’ values are the names of the categories. You can think of \\(X1_{i}\\) as toggling on and off the \\(\\beta_{1}\\) effect: If it has a value of 0 for a data point it means that \\(\\beta_{1}\\) will not impact the response. The response will be will be \\(\\beta_{0}\\). If it has a value 1 then \\(\\beta_{1}\\) will change the response to \\(\\beta_{0}\\) + \\(\\beta_{1}\\) \\(\\beta_{1}\\) is thus the difference between the group means. A graphical representation of the terms in a linear model when the explanatory variable is categorical with two groups is given in Figure 5.1. Figure 5.1: A linear model when the explanatory variable is categorical with two groups annotated with the terms used in linear modelling. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters are indicated: \\(\\beta_{0}\\) is the mean of group A and \\(\\beta_{1}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group B. Compare to Figure 2.2. 5.4 Applying and interpreting lm() The lm() function is applied to this example as follows: mod &lt;- lm(data = csativa, omega ~ plant) This can be read as: fit a linear of model of omega content explained by plant type. Notice that the model formula is the same in both the t.test() and the lm() functions. Printing mod to the console gives us the estimated model parameters (coefficients): mod # # Call: # lm(formula = omega ~ plant, data = csativa) # # Coefficients: # (Intercept) plantwild # 49.47 6.95 The equation for the model is: \\(omega\\) = 49.465 + 6.947\\(plantwild\\) The first group of plant is modif so \\(\\beta_{0}\\) is the mean of the modified plants. \\(\\beta_{1}\\) is the coefficient labelled plantwild. In R, the coefficients are consistently named like this: variable name followed by the value without spaces. It means when the variable plant takes the value wild, \\(\\beta_{1}\\) must be added to \\(\\beta_{0}\\) Thus, the mean omega 3 in the modified plants is 49.465 units and that in the wild type plants is 49.465 + 6.947 = 56.412 units. More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # lm(formula = omega ~ plant, data = csativa) # # Residuals: # Min 1Q Median 3Q Max # -15.872 -3.703 -0.964 4.460 16.918 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 49.465 0.977 50.64 &lt; 2e-16 *** # plantwild 6.947 1.381 5.03 2.2e-06 *** # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 6.91 on 98 degrees of freedom # Multiple R-squared: 0.205, Adjusted R-squared: 0.197 # F-statistic: 25.3 on 1 and 98 DF, p-value: 2.23e-06 The Coefficients table gives the estimated \\(\\beta_{0}\\) and \\(\\beta_{1}\\) again but along with their standard errors and tests of whether the estimates differ from zero. The estimated mean of the modified plants is 49.465 \\(\\pm\\) 0.977 and this differs significantly from zero (\\(p\\) &lt; 0.001). The estimated difference between the modified and wild type plants is 6.947 \\(\\pm\\) 1.381 and also differs significantly from zero (\\(p\\) &lt; 0.001). The fact that this value is positive tells us that the wild type plants have a higher mean. The proportion of the variance in the omega which is explained by the model is 0.205 and this is a significant proportion of that variance (\\(p\\) &lt; 0.001). As was true for single linear regression, the p-value for the model and the p-value for the difference between the means are the same because there is only one parameter in the model after the intercept. Replacing the terms shown in Figure 5.1 with the values in this example gives us 5.2. Figure 5.2: The annotated model with the values from the Omega 3 content of Cannabis sativa example. The measured response values are in pink, the predictions are in green, and the residuals, are in blue. One example of a measured value, a predicted value and the residual is shown for a wild type individual. The estimated model parameters are indicated: \\(\\beta_{0}\\), the mean of the modified plants, is 49.465 and \\(\\beta_{1}\\) is 6.947. Thus the mean of wildtype plants is 49.465 + 6.947 = 56.412 units. Compare to Figure 5.1. 5.5 Getting predictions from the model We already have the predictions for all possible values of the explanatory variable because there are only two! However the code for using predict is included here because it will make it easier to understand more complex examples later. We need to create a dataframe of values for which we want predictions and pass it as an argument to the predict() function. To create a dataframe with one column of Plant values: predict_for &lt;- data.frame(plant = c(&quot;modif&quot;, &quot;wild&quot;)) Remember! The variable and its values have to exactly match those in the model. The to get the predicted omega content for the two plant types: predict_for$pred &lt;- predict(mod, newdata = predict_for) glimpse(predict_for) # Rows: 2 # Columns: 2 # $ plant &lt;chr&gt; &quot;modif&quot;, &quot;wild&quot; # $ pred &lt;dbl&gt; 49.5, 56.4 5.6 Checking assumptions The two assumptions of the model can be checked using diagnostic plots. The Q-Q plot is obtained with: plot(mod, which = 2) The residual seem to be normally distributed. Let’s look at the Residuals vs Fitted plot: plot(mod, which = 1) We get these two columns of points because the explanatory variable, plant, is categorical so the fitted - or predicted - values are just two means. In my view, the variance looks higher in the group with the higher mean (on the right). 5.7 Creating a figure #summarise the data ggplot() + geom_jitter(data = csativa, aes(x = plant, y = omega), width = 0.25, colour = &quot;grey&quot;) + geom_errorbar(data = csativa_summary, aes(x = plant, ymin = mean, ymax = mean), width = .3) + geom_errorbar(data = csativa_summary, aes(x = plant, ymin = mean - se, ymax = mean + se), width = .5) + geom_segment(aes(x = 1, y = 75, xend = 2, yend = 75), size = 1) + geom_segment(aes(x = 1, y = 75, xend = 1, yend = 73), size = 1) + geom_segment(aes(x = 2, y = 75, xend = 2, yend = 73), size = 1) + annotate(&quot;text&quot;, x = 1.5, y = 77, label = &quot;***&quot;, size = 6) + scale_x_discrete(labels = c(&quot;Modified&quot;, &quot;Wild Type&quot;), name = &quot;Plant type&quot;) + scale_y_continuous(name = &quot;Amount of Omega 3 (units)&quot;, expand = c(0, 0), limits = c(0, 90)) + theme_classic() 5.8 Reporting the results The genetic modification was unsuccessful with wild type plants (\\(\\bar{x} \\pm s.e.\\): 56.412 \\(\\pm\\) 1.11 units) have significantly higher omega 3 than modified plants(49.465 \\(\\pm\\) 0.823 units) (\\(t\\) = 5.029; \\(d.f.\\) = 98; \\(p\\) &lt; 0.001). See figure 5.3. Figure 5.3: Mean Omega 3 content of wild type and genetically modified Cannabis sativa. Error bars are \\(\\pm 1 S.E.\\). *** significant difference at the \\(p &lt; 0.001\\) level. "],
["one-way-anova-revisit.html", "Chapter 6 One-way ANOVA revisited 6.1 Introduction to the example 6.2 aov() output reminder 6.3 Post-hoc testing for aov() 6.4 One-way ANOVAs as linear models 6.5 Applying and interpreting lm() 6.6 Getting predictions from the model 6.7 Checking assumptions 6.8 Post-hoc testing for lm() 6.9 Creating a figure 6.10 Reporting the results", " Chapter 6 One-way ANOVA revisited In this chapter we again consider an example with one categorical explanatory variable. However, this time it has more than two groups (or levels). We first use the familiar aov() function to carry out a one-way ANOVA and then use that understanding to help us understand the output of lm(). We will also make predictions from the model and report on our results. 6.1 Introduction to the example Figure 6.1: Baby Weddell Seals are very cute. By Photo © Samuel Blanc, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=3877642 The myoglobin concentration of skeletal muscle (in grams per kilogram of muscle) for three species of seal (see Figure 6.1) is given in seal.txt. species myoglobin Harbour Seal 49.7 Harbour Seal 51.0 Harbour Seal 41.6 Harbour Seal 45.6 Harbour Seal 39.4 Harbour Seal 43.1 Harbour Seal 55.7 Harbour Seal 66.1 Harbour Seal 56.0 Harbour Seal 47.0 Harbour Seal 51.5 Harbour Seal 50.9 Harbour Seal 53.4 Harbour Seal 35.3 Harbour Seal 45.2 Harbour Seal 38.4 Harbour Seal 50.5 Harbour Seal 49.4 Harbour Seal 41.6 Harbour Seal 41.5 Harbour Seal 45.8 Harbour Seal 48.1 Harbour Seal 57.3 Harbour Seal 61.2 Harbour Seal 48.8 Harbour Seal 62.2 Harbour Seal 38.0 Harbour Seal 40.6 Harbour Seal 67.3 Harbour Seal 48.5 Weddell Seal 55.4 Weddell Seal 40.1 Weddell Seal 46.3 Weddell Seal 29.8 Weddell Seal 52.5 Weddell Seal 37.4 Weddell Seal 42.8 Weddell Seal 51.4 Weddell Seal 48.5 Weddell Seal 44.0 Weddell Seal 58.0 Weddell Seal 45.4 Weddell Seal 37.1 Weddell Seal 39.3 Weddell Seal 45.1 Weddell Seal 51.1 Weddell Seal 38.1 Weddell Seal 36.5 Weddell Seal 49.4 Weddell Seal 62.0 Weddell Seal 45.7 Weddell Seal 57.0 Weddell Seal 42.6 Weddell Seal 40.0 Weddell Seal 31.9 Weddell Seal 42.7 Weddell Seal 46.0 Weddell Seal 39.0 Weddell Seal 50.1 Weddell Seal 34.4 Bladdernose Seal 56.2 Bladdernose Seal 48.4 Bladdernose Seal 37.8 Bladdernose Seal 42.8 Bladdernose Seal 27.0 Bladdernose Seal 43.1 Bladdernose Seal 42.4 Bladdernose Seal 29.9 Bladdernose Seal 42.3 Bladdernose Seal 58.1 Bladdernose Seal 32.2 Bladdernose Seal 38.4 Bladdernose Seal 52.6 Bladdernose Seal 53.9 Bladdernose Seal 42.3 Bladdernose Seal 46.4 Bladdernose Seal 44.6 Bladdernose Seal 49.0 Bladdernose Seal 40.2 Bladdernose Seal 41.4 Bladdernose Seal 38.6 Bladdernose Seal 35.1 Bladdernose Seal 48.2 Bladdernose Seal 33.2 Bladdernose Seal 38.4 Bladdernose Seal 26.0 Bladdernose Seal 50.0 Bladdernose Seal 42.6 Bladdernose Seal 47.0 Bladdernose Seal 41.6 The data were collected to determine whether muscle myoglobin differed between species. There are 2 variables. seal is the explanatory variable; it is categorical with 3 levels, Bladdernose Seal, Harbour Seal and Weddell Seal. myoglobin, a continuous variable, is the response. We can use the read_delim() function to import the data and visualise it with ggplot(). seal &lt;- read_delim(&quot;data-raw/seal.txt&quot;, delim = &quot; &quot;) # create a rough plot of the data ggplot(data = seal, aes(x = species, y = myoglobin)) + geom_violin() Harbour Seals seem to have higher myoglobin than the other two species and the variance in myoglobin for the three species looks about the same. Let’s create a summary of the data that will be useful for plotting later: seal_summary &lt;- seal %&gt;% group_by(species) %&gt;% summarise(mean = mean(myoglobin), std = sd(myoglobin), n = length(myoglobin), se = std/sqrt(n)) species mean std n se Bladdernose Seal 42.3 8.02 30 1.46 Harbour Seal 49.0 8.25 30 1.51 Weddell Seal 44.7 7.85 30 1.43 Our summary confirms that there are thirty individuals of each species and that highest mean is for Harbour Seals and the lowest is for Bladdernose Seals. The variance within each species is similar. 6.2 aov() output reminder The aov() function requires a model formula, myoglobin ~ species, in the familiar format. We also specify the data argument to indicate where the species and myoglobin variables can be found: mod &lt;- aov(data = seal, myoglobin ~ species) The output of the summary() function gives us an ANOVA test: summary(mod) # Df Sum Sq Mean Sq F value Pr(&gt;F) # species 2 692 346 5.35 0.0064 ** # Residuals 87 5627 65 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There was a significant difference in myoglobin concentration between seal species (ANOVA: \\(F\\) = 5.352; \\(d.f.\\) = 2, 87; \\(p\\) = 0.006). We need a post-hoc multiple comparison test to discover which pairs of means differ significantly. 6.3 Post-hoc testing for aov() A commonly applied multiple comparison test applied after an significant ANOVA result is the Tukey Honest Significant Difference test: TukeyHSD(mod) # Tukey multiple comparisons of means # 95% family-wise confidence level # # Fit: aov(formula = myoglobin ~ species, data = seal) # # $species # diff lwr upr p adj # Harbour Seal-Bladdernose Seal 6.69 1.74 11.646 0.005 # Weddell Seal-Bladdernose Seal 2.34 -2.61 7.296 0.499 # Weddell Seal-Harbour Seal -4.35 -9.30 0.602 0.097 The p-value, adjusted for multiple comparisons is given in the p adj column. In this case, only one of the three pairwise comparisons is significant. Harbour Seals, with the highest myoglobin concentrations (\\(\\bar{x} \\pm s.e.\\): 49.01 \\(\\pm\\) 1.507) ) were significantly higher than Bladdernose Seals with the lowest (\\(\\bar{x} \\pm s.e.\\): 42.316 \\(\\pm\\) 1.464). The comparisons being made are known as contrasts and this terminology will appear later. 6.4 One-way ANOVAs as linear models The equation for a one-way ANOVA test is an extension of equation (5.1) for a t-test. It has the same form but additional parameters. If there are three groups, the model is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i} \\tag{6.1} \\end{equation}\\] The parameter \\(\\beta_{0}\\), the intercept, is the value of the response when the categorical explanatory is at its “lowest” level. \\(X1_{i}\\) and \\(X2_{i}\\) are the dummy explanatory variables which take a value of 0 or 1 to toggle on and off the effects of \\(\\beta_{1}\\) and \\(\\beta_{2}\\) respectively. \\(\\beta_{1}\\) is the difference between the mean of the group represented by the intercept and the next group and \\(\\beta_{2}\\) is the difference between the mean of the group represented by the intercept and the group after that. An additional parameter and dummy variable are added for each additional group so for four groups the equation is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i}+\\beta_{3}X3_{i} \\tag{6.2} \\end{equation}\\] A graphical representation of the terms in a linear model when the explanatory variable is categorical with four groups is given in Figure 6.2. Figure 6.2: A linear model when the explanatory variable is categorical with four groups annotated with the terms used in linear modelling. The measured response values are in pink, the predictions are in green, and the differences between these, known as the residuals, are in blue. The estimated model parameters are indicated: \\(\\beta_{0}\\) is the mean of group A; \\(\\beta_{1}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group B; \\(\\beta_{2}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group C; and \\(\\beta_{3}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group D. In this figure, \\(\\beta_{1}\\) and \\(\\beta_{2}\\) are positive and \\(\\beta_{3}\\) is negative. Compare to Figure 2.2. All the \\(\\beta\\) values are given relative to \\(\\beta_{0}\\). Their sign indicates whether a group mean is bigger (positive) or smaller (negative) than the intercept. 6.5 Applying and interpreting lm() The lm() function is applied to the seal example as follows: mod &lt;- lm(data = seal, myoglobin ~ species) Printing mod to the console gives us the estimated model parameters (coefficients): mod # # Call: # lm(formula = myoglobin ~ species, data = seal) # # Coefficients: # (Intercept) speciesHarbour Seal speciesWeddell Seal # 42.32 6.69 2.34 The equation for the model is: \\(myoglobin\\) = 42.316 + 6.694\\(speciesHarbour Seal\\) + 2.344\\(speciesWeddell Seal\\) The first group of seal is Bladdernose Seal so \\(\\beta_{0}\\) is the mean of the Bladdernose seals. \\(\\beta_{1}\\) is the coefficient labelled speciesHarbour Seal and means when the variable species takes the value Harbour Seal, \\(\\beta_{1}\\) must be added to \\(\\beta_{0}\\). The last parameter, \\(\\beta_{2}\\), is the coefficient labelled speciesWeddell Seal and means when the variable species takes the value Weddell Seal, \\(\\beta_{2}\\) must be added to \\(\\beta_{0}\\). The mean myoglobin in Bladdernose seals is 42.316 kg g-1, that in Harbour Seals is 42.316 + 6.694 = 49.01 kg g-1 and in Weddell Seals is 42.316 + 2.344 = 44.66kg g-1. More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # lm(formula = myoglobin ~ species, data = seal) # # Residuals: # Min 1Q Median 3Q Max # -16.306 -5.578 -0.036 5.240 18.250 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 42.32 1.47 28.82 &lt;2e-16 *** # speciesHarbour Seal 6.69 2.08 3.22 0.0018 ** # speciesWeddell Seal 2.34 2.08 1.13 0.2620 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 8.04 on 87 degrees of freedom # Multiple R-squared: 0.11, Adjusted R-squared: 0.0891 # F-statistic: 5.35 on 2 and 87 DF, p-value: 0.00643 The Coefficients table gives the estimated \\(\\beta_{0}\\), \\(\\beta_{1}\\) and \\(\\beta_{2}\\) again but along with their standard errors and tests of whether the estimates differ from zero. The estimated mean of the Bladdernose seals is 42.316 \\(\\pm\\) 1.468 kg g1 and this differs significantly from zero (\\(p\\) &lt; 0.001). The estimated difference between the Bladdernose and Harbour seals is 6.694 \\(\\pm\\) 2.077 and also differs significantly from zero (\\(p\\) = 0.002). The estimated difference between the Bladdernose and Weddell seals, 2.344 \\(\\pm\\) 2.077 kg g1, does not differ significantly from zero (\\(p\\) = 0.262). The fact that both parameters are positive tells us both of the other two species have higher means than Bladdernose. The proportion of the variance in the omega which is explained by the model is 0.11 and this is a significant proportion of that variance (\\(p\\) = 0.006). This is the first time we have a model where the p-value for the model and the p-values for the \\(\\beta\\) parameters differ. This is because we are fitting two parameters after the intercept. Replacing the terms shown in Figure 6.2 with the values in this example gives us 6.3. Figure 6.3: The annotated model with the values from the Seal species example. The measured response values are in pink, the predictions are in green, and the residuals, are in blue. One example of a measured value, a predicted value and the residual is shown for an individual harbour seal. The estimated model parameters are indicated: \\(\\beta_{0}\\), the mean of the Bladdernose Seals, is 42.316 kg g1; \\(\\beta_{1}\\) is 6.694 thus the mean of Harbour Seals 42.316 + 6.694 = 49.01 kg g^-1; and \\(\\beta_{2}\\) is 2.344 thus the mean of Weddell Seals 42.316 + 2.344 = 49.01 kg g-1. Compare to Figure 6.2. 6.6 Getting predictions from the model We already have the predictions for all possible values of the explanatory variable because it is categorical. However, the code for using predict is included here, as it was in the last chapter, because it will make it easier to understand more complex examples later. We need to create a dataframe of values for which we want predictions and pass it as an argument to the predict() function. To create a dataframe with one column of Species values: predict_for &lt;- data.frame(species = c(&quot;Bladdernose Seal&quot;, &quot;Harbour Seal&quot;, &quot;Weddell Seal&quot;)) Remember! The variable and its values have to exactly match those in the model. The to get the predicted myoglobin content for the three species: predict_for$pred &lt;- predict(mod, newdata = predict_for) 6.7 Checking assumptions The two assumptions of the model can be checked using diagnostic plots. The Q-Q plot is obtained with: plot(mod, which = 2) The residual seem to be normally distributed. Let’s look at the Residuals vs Fitted plot: plot(mod, which = 1) The residuals are equally spread around a horizontal line; the assumptions seem to be met. 6.8 Post-hoc testing for lm() Instead of using the TukeyHSD() we will use the glht() (generalized linear hypothesis test) function from the multcomp package (Hothorn, Bretz, and Westfall 2008). This is function that can be applied more widely than TukeyHSD(). It provides multiple comparisons for linear models, generalised linear models and linear mixed effects models. This tremendous flexibility comes at some cost and the arguments for the glht() function are relatively complex. However, you don’t need a full understanding to be able to use it. glht() requires our species variable to be a factor so our first task is to transform that variable and rebuild our model: seal$species &lt;- factor(seal$species) mod &lt;- lm(data = seal, myoglobin ~ species) Then load the package: library(multcomp) We have to specify our contrasts as a matrix with the linfct (linear functions) argument and there is a multiple comparisons function, mcp(), to help. This is the whole command: mod_mc &lt;- glht(mod, linfct = mcp(species = &quot;Tukey&quot;)) You can read this as “do all of the pairwise comparisons between each species in the model mod using the Tukey test”. We view the results with summary(): summary(mod_mc) # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = myoglobin ~ species, data = seal) # # Linear Hypotheses: # Estimate Std. Error t value Pr(&gt;|t|) # Harbour Seal - Bladdernose Seal == 0 6.69 2.08 3.22 0.005 ** # Weddell Seal - Bladdernose Seal == 0 2.34 2.08 1.13 0.499 # Weddell Seal - Harbour Seal == 0 -4.35 2.08 -2.09 0.097 . # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) The results are the same as for using TukeyHSD() as we have done the same tests using a different function. You can see what a contrasts matrix looks like by looking at the linfct variable of the glht object. You don’t need it now but in the future you may need to specify your own contrasts matrices so let’s have a quick look to aid your journey towards understanding: mod_mc$linfct # (Intercept) speciesHarbour Seal # Harbour Seal - Bladdernose Seal 0 1 # Weddell Seal - Bladdernose Seal 0 0 # Weddell Seal - Harbour Seal 0 -1 # speciesWeddell Seal # Harbour Seal - Bladdernose Seal 0 # Weddell Seal - Bladdernose Seal 1 # Weddell Seal - Harbour Seal 1 # attr(,&quot;type&quot;) # [1] &quot;Tukey&quot; It is matrix with a column for each parameter, in order) and a row for each contrast containing only 0s, 1s and -1s. The rows are named. The numbers are how the model parameters are needed to make the contrast and these can be understood by considering how the group means relate to the parameters. Bladdernose mean is \\(\\beta_{0}\\) Harbour mean is \\(\\beta_{0} + \\beta_{1}\\) Weddell mean is \\(\\beta_{0} + \\beta_{2}\\) Therefore: Harbour Seal - Bladdernose Seal is: \\(\\beta_{0} + \\beta_{1} - \\beta_{0} = \\beta_{1}\\) and there is a one in the speciesHarbour Seal column and zeros else where Weddell Seal - Bladdernose Seal is: \\(\\beta_{0} + \\beta_{2} - \\beta_{0} = \\beta_{2}\\) and there is a one in the speciesWeddell Seal column and zeros else where Weddell Seal - Harbour Seal is: \\(\\beta_{0} + \\beta_{2} - (\\beta_{0} + \\beta_{1}) = \\beta_{2} - \\beta_{1}\\) and there is a 1 in the the speciesWeddell Seal column and a -1 in the speciesHarbour Seal column. 6.9 Creating a figure #summarise the data ggplot() + geom_jitter(data = seal, aes(x = species, y = myoglobin), width = 0.25, colour = &quot;grey&quot;) + geom_errorbar(data = seal_summary, aes(x = species, ymin = mean, ymax = mean), width = .3) + geom_errorbar(data = seal_summary, aes(x = species, ymin = mean - se, ymax = mean + se), width = .5) + geom_segment(aes(x = 1, y = 71, xend = 3, yend = 71), size = 1) + geom_segment(aes(x = 1, y = 71, xend = 1, yend = 69), size = 1) + geom_segment(aes(x = 3, y = 71, xend = 3, yend = 69), size = 1) + annotate(&quot;text&quot;, x = 2, y = 73, label = &quot;**&quot;, size = 6) + scale_x_discrete(name = &quot;Species&quot;) + scale_y_continuous(name = expression(&quot;Myoglobin concentration g &quot;*Kg^{-1}), expand = c(0, 0), limits = c(0, 75)) + theme_classic() 6.10 Reporting the results There is a significant difference in myoglobin concentration between Seal species (ANOVA: \\(F\\) = 5.352; \\(d.f.\\) = 2, 87; \\(p\\) = 0.006). Post-hoc testing revealed that difference to be between the Harbour Seal with the highest myoglobin concentrations (\\(\\bar{x} \\pm s.e.\\): 49.01 \\(\\pm\\) 1.507) ) and the Bladdernose Seal(= 0.005) with the lowest (\\(\\bar{x} \\pm s.e.\\): 42.316 \\(\\pm\\) 1.464). See figure 6.4. Figure 6.4: Muscle myoglobin content of three seal species. References "],
["two-way-anova-revisit.html", "Chapter 7 Two-way ANOVA revisited 7.1 Introduction to the example 7.2 aov() output reminder 7.3 Post-hoc testing for aov 7.4 Two-way ANOVAs as linear models 7.5 Applying and interpreting lm() 7.6 Getting predictions from the model 7.7 Checking assumptions 7.8 Post-hoc testing for lm() 7.9 Creating a figure 7.10 Reporting the results", " Chapter 7 Two-way ANOVA revisited In this chapter we turn our attention to designs with two categorical explanatory variables. We first use the familiar aov() function to carry out a two-way ANOVA and then use our understanding to help us interpret the output of lm(). We will also make predictions from the model and report on our results. 7.1 Introduction to the example Researchers have collected live specimens of two species of periwinkle (see Figure 7.1) from sites in northern England in the Spring and Summer. They take a measure of the gut parasite load by examining a slide of gut contents. The data are in periwinkle.txt. Figure 7.1: Periwinkles are marine gastropod molluscs (slugs and snails). A) Littorina brevicula (PD files - Public Domain, https://commons.wikimedia.org/w/index.php?curid=30577419) B) Littorina littorea. (photographed by Guttorm Flatabø (user:dittaeva). - Photograph taken with an Olympus Camedia C-70 Zoom digital camera. Metainformation edited with Irfanview, possibly cropped with jpegcrop., CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=324769 para season species 58 Spring Littorina brevicula 51 Spring Littorina brevicula 54 Spring Littorina brevicula 39 Spring Littorina brevicula 65 Spring Littorina brevicula 67 Spring Littorina brevicula 60 Spring Littorina brevicula 54 Spring Littorina brevicula 47 Spring Littorina brevicula 66 Spring Littorina brevicula 51 Spring Littorina brevicula 43 Spring Littorina brevicula 62 Spring Littorina brevicula 55 Spring Littorina brevicula 58 Spring Littorina brevicula 43 Spring Littorina brevicula 69 Spring Littorina brevicula 71 Spring Littorina brevicula 64 Spring Littorina brevicula 58 Spring Littorina brevicula 51 Spring Littorina brevicula 70 Spring Littorina brevicula 55 Spring Littorina brevicula 47 Spring Littorina brevicula 54 Spring Littorina brevicula 43 Spring Littorina littorea 63 Spring Littorina littorea 58 Spring Littorina littorea 61 Spring Littorina littorea 63 Spring Littorina littorea 45 Spring Littorina littorea 45 Spring Littorina littorea 77 Spring Littorina littorea 57 Spring Littorina littorea 49 Spring Littorina littorea 72 Spring Littorina littorea 47 Spring Littorina littorea 78 Spring Littorina littorea 77 Spring Littorina littorea 71 Spring Littorina littorea 69 Spring Littorina littorea 66 Spring Littorina littorea 76 Spring Littorina littorea 75 Spring Littorina littorea 54 Spring Littorina littorea 62 Spring Littorina littorea 58 Spring Littorina littorea 76 Spring Littorina littorea 68 Spring Littorina littorea 84 Spring Littorina littorea 61 Summer Littorina brevicula 70 Summer Littorina brevicula 68 Summer Littorina brevicula 67 Summer Littorina brevicula 88 Summer Littorina brevicula 64 Summer Littorina brevicula 69 Summer Littorina brevicula 70 Summer Littorina brevicula 56 Summer Littorina brevicula 80 Summer Littorina brevicula 80 Summer Littorina brevicula 53 Summer Littorina brevicula 56 Summer Littorina brevicula 101 Summer Littorina brevicula 81 Summer Littorina brevicula 88 Summer Littorina brevicula 67 Summer Littorina brevicula 73 Summer Littorina brevicula 75 Summer Littorina brevicula 74 Summer Littorina brevicula 68 Summer Littorina brevicula 89 Summer Littorina brevicula 75 Summer Littorina brevicula 74 Summer Littorina brevicula 76 Summer Littorina brevicula 66 Summer Littorina littorea 61 Summer Littorina littorea 51 Summer Littorina littorea 67 Summer Littorina littorea 78 Summer Littorina littorea 79 Summer Littorina littorea 69 Summer Littorina littorea 76 Summer Littorina littorea 86 Summer Littorina littorea 97 Summer Littorina littorea 64 Summer Littorina littorea 68 Summer Littorina littorea 65 Summer Littorina littorea 49 Summer Littorina littorea 62 Summer Littorina littorea 57 Summer Littorina littorea 70 Summer Littorina littorea 62 Summer Littorina littorea 80 Summer Littorina littorea 81 Summer Littorina littorea 85 Summer Littorina littorea 77 Summer Littorina littorea 61 Summer Littorina littorea 59 Summer Littorina littorea 66 Summer Littorina littorea The data were collected to determine whether there was an effect of season or species on parasite load and whether these effects were independent. There are 3 variables: species and seasonare categorical explanatory variables, each with two levels; para, a continuous variable, is the response. We can use the read_delim() function to import the data. periwinkle &lt;- read_delim(&quot;data-raw/periwinkle.txt&quot;, delim = &quot;\\t&quot;) When visualising this data with ggplot() we need to account for both explanatory variables. We can map one to the x-axis and the other to a different aesthetic. Using the fill aesthetic works well for violin plots. ggplot(data = periwinkle, aes(x = season, y = para, fill = species)) + geom_violin() Parasite load seems to be higher for both species in the summer and that effect looks bigger in L.brevicula - it has the lowest spring mean but the highest summer mean. Let’s create a summary of the data that will be useful for plotting later: peri_summary &lt;- periwinkle %&gt;% group_by(season, species) %&gt;% summarise(mean = mean(para), sd = sd(para), n = length(para), se = sd / sqrt(n)) season species mean sd n se Spring Littorina brevicula 56.5 8.88 25 1.78 Spring Littorina littorea 63.8 11.92 25 2.38 Summer Littorina brevicula 72.9 11.24 25 2.25 Summer Littorina littorea 69.4 11.44 25 2.29 The summary confirms both species have a higher mean in the summer and that the difference between the species is reversed - L.brevicula \\(-\\) L.littorea is -7.28 in the spring but 3.48 in summer. 7.2 aov() output reminder The aov() function requires a model formula which includes both explanatory variables and the interaction between them in the familiar format: para ~ season * season . We also specify the data argument to indicate where the variables can be found: mod &lt;- aov(data = periwinkle, para ~ season * species) The output of the summary() function gives us an ANOVA test: summary(mod) # Df Sum Sq Mean Sq F value Pr(&gt;F) # season 1 3058 3058 25.58 2e-06 *** # species 1 90 90 0.75 0.387 # season:species 1 724 724 6.05 0.016 * # Residuals 96 11477 120 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There was a significantly greater number of parasites in the Summer than the Spring (ANOVA: \\(F\\) = 25.58; \\(d.f.\\) = 1, 96; \\(p\\) &lt; 0.001). There was no difference between the species when averaged across the seasons but there was significant interaction (ANOVA: \\(F\\) = 6.053; \\(d.f.\\) = 1, 96; \\(p\\) = 0.016) between season and species with higher numbers infecting L.littorea in the Spring whilst L.brevicula was more heavily parasitized in the Summer. We need a post-hoc test to discover which comparisons are significant. 7.3 Post-hoc testing for aov Tukey Honest Significant Difference test is carried out with: TukeyHSD(mod) # Tukey multiple comparisons of means # 95% family-wise confidence level # # Fit: aov(formula = para ~ season * species, data = periwinkle) # # $season # diff lwr upr p adj # Summer-Spring 11.1 6.72 15.4 0 # # $species # diff lwr upr p adj # Littorina littorea-Littorina brevicula 1.9 -2.44 6.24 0.387 # # $`season:species` # diff lwr upr p adj # Summer:Littorina brevicula-Spring:Littorina brevicula 16.44 8.354 24.53 0.000 # Spring:Littorina littorea-Spring:Littorina brevicula 7.28 -0.806 15.37 0.093 # Summer:Littorina littorea-Spring:Littorina brevicula 12.96 4.874 21.05 0.000 # Spring:Littorina littorea-Summer:Littorina brevicula -9.16 -17.246 -1.07 0.020 # Summer:Littorina littorea-Summer:Littorina brevicula -3.48 -11.566 4.61 0.675 # Summer:Littorina littorea-Spring:Littorina littorea 5.68 -2.406 13.77 0.263 The parasite load for L.brevicula increases significantly between spring and summer (\\(p\\) &lt; 0.001) while that for L.littorea does not. Other significant comparisons are: the spring load of L.brevicula is lower than the summer load of L.littorea (\\(p\\) &lt; 0.001); and summer load of L.brevicula is higher than the spring load of L.littorea (\\(p\\) = 0.02). 7.4 Two-way ANOVAs as linear models The equation for a two-way ANOVA test is an extension of equation (6.1) for a one-way ANOVA test. It has the same form but an additional parameter. If there are two groups in each explanatory variable, the model is: \\[\\begin{equation} E(y_{i})=\\beta_{0}+\\beta_{1}X1_{i}+\\beta_{2}X2_{i}+\\beta_{3}X1_{i}X2_{i} \\tag{6.1} \\end{equation}\\] The intercept, \\(\\beta_{0}\\) is the value of the response when both categorical explanatory variables are at their “lowest” level. \\(X1_{i}\\) is a dummy explanatory variable which indicates the first explanatory variable changing to its second level. It toggles on and off the effects of \\(\\beta_{1}\\). \\(X2_{i}\\) is a dummy explanatory variable which indicates the second explanatory variable changing to its second level and toggles on and off the effects of \\(\\beta_{2}\\). \\(\\beta_{3}\\) is the interaction effect. If \\(X1_{i}\\) and \\(X2_{i}\\) are both 1 \\(\\beta_{3}\\) is the extra effect of that combination above the sum of \\(\\beta_{1}+\\beta_{2}\\) The number of parameters in a two-way ANOVA design is: the number of levels in one explanatory \\(\\times\\) the number of levels in the other explanatory. If each explanatory have three levels, there would be nine \\(\\beta s\\) A graphical representation of the terms in a linear model when there are two explanatory variables each with two groups (or levels) is given in Figure 7.2. Figure 7.2: A linear model with two explanatory variables each with two levels. ‘Variable 1’ has levels ‘A’ and ‘B’ and ‘Variable 2’ has levels ‘a’ and ‘b’. Thus there are 2 \\(\\times\\) 2 = 4 groups: Aa, Ab, Ba and Bb. The measured response values are in pink and the predictions are in green. The residuals are not indicated. The estimated model parameters are indicated: \\(\\beta_{0}\\) is the mean of group Aa; \\(\\beta_{1}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group Ab; \\(\\beta_{2}\\) is what has to be added to \\(\\beta_{0}\\) to get the mean of group Ba; and \\(\\beta_{3}\\) is what has to be added to \\(\\beta_{0}\\) in addition to \\(\\beta_{1}\\) and \\(\\beta_{2}\\) to get the mean of group Bb. In this figure, all parameters are positive. Compare to Figure 2.2. The intercept, \\(\\beta_{0}\\)is the response when both explanatory variable is at their first group and all the other \\(\\beta s\\) are given relative to this. The interaction parameters give the effect of the combination in addition to the sum of their independent effects 7.5 Applying and interpreting lm() The lm() function is applied to the periwinkle example as follows: mod &lt;- lm(data = periwinkle, para ~ season * species) Printing mod to the console gives us the estimated model parameters (coefficients): mod # # Call: # lm(formula = para ~ season * species, data = periwinkle) # # Coefficients: # (Intercept) seasonSummer # 56.48 16.44 # speciesLittorina littorea seasonSummer:speciesLittorina littorea # 7.28 -10.76 The equation for the model is: \\(para\\) = 56.48 + 16.44\\(speciesLittorina littorea\\) + 7.28\\(seasonSummer:speciesLittorina littorea\\) The first group of season is Spring and the first group of species is Littorina breviculaso \\(\\beta_{0}\\) is the mean of L.brevicula in the spring, 56.48. \\(\\beta_{1}\\) is the coefficient labelled seasonSummer and means when the variable season takes the value Summer, \\(\\beta_{1}\\) must be added to \\(\\beta_{0}\\) - the mean of L.brevicula in the summer is \\(\\beta_{0}+\\beta_{1}\\) = 56.48 \\(+\\) 16.44 \\(=\\) 72.92. The coefficient labelled speciesLittorina littorea is \\(\\beta_{2}\\). When species becomes Littorina littorea, \\(\\beta_{1}\\) must be added to \\(\\beta_{0}\\) thus the mean of L.littorea in spring is \\(\\beta_{0}+\\beta_{2}\\) = 56.48 \\(+\\) 7.28 \\(=\\) 63.76. If both season becomes Summer and species becomes Littorina littorea you would expect the effect to be \\(\\beta_{0}+\\beta_{1}+\\beta_{2}\\). The coefficient labelled seasonSummer:speciesLittorina littorea, \\(\\beta_{3}\\) is the effect that is additional to that sum. An interaction is when combined effect of two variables is more than just adding the independent effects. The mean of L.littorea in summer is \\(\\beta_{0}+\\beta_{1}+\\beta_{2}+\\beta_{3}\\) = 56.48 \\(+\\) rb1 \\(+\\) 7.28 \\(+\\) rb3 \\(=\\) 69.44. More information including statistical tests of the model and its parameters is obtained by using summary(): summary(mod) # # Call: # lm(formula = para ~ season * species, data = periwinkle) # # Residuals: # Min 1Q Median 3Q Max # -20.76 -6.13 -1.10 8.12 28.08 # # Coefficients: # Estimate Std. Error t value Pr(&gt;|t|) # (Intercept) 56.48 2.19 25.83 &lt; 2e-16 *** # seasonSummer 16.44 3.09 5.32 6.9e-07 *** # speciesLittorina littorea 7.28 3.09 2.35 0.021 * # seasonSummer:speciesLittorina littorea -10.76 4.37 -2.46 0.016 * # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # # Residual standard error: 10.9 on 96 degrees of freedom # Multiple R-squared: 0.252, Adjusted R-squared: 0.229 # F-statistic: 10.8 on 3 and 96 DF, p-value: 3.55e-06 The Coefficients table gives the estimated \\(\\beta_{0}\\), \\(\\beta_{1}\\), \\(\\beta_{2}\\) and \\(\\beta_{3}\\) again but along with their standard errors and tests of whether the estimates differ from zero. The estimated mean of L.brevicula in the spring is 56.48 \\(\\pm\\) 2.187 and this differs significantly from zero (\\(p\\) &lt; 0.001). The estimated difference between the L.brevicula in the spring and L.brevicula in the summer 16.44 \\(\\pm\\) 3.093 and also differs significantly from zero (\\(p\\) &lt; 0.001). The estimated difference between L.brevicula in the spring and L.littorea in the spring, 7.28 \\(\\pm\\) 3.093 differs significantly from zero (\\(p\\) = 0.021). The proportion of the variance in parasite load explained by the model is 0.252 and this is a significant proportion of that variance (\\(p\\) &lt; 0.001). As we are fitting three parameters in addition to the intercept our p-value for the model and the p-values for the \\(\\beta\\) parameters differ. This was also true for one-way ANOVA. Replacing the terms shown in Figure @ref(fig:gen_two_way) with the values in this example gives us 7.3. Figure 7.3: The annotated model with the values from the parasite load of preiwinkle example. The measured response values are in pink and the predictions are in green. The estimated model parameters are indicated: \\(\\beta_{0}\\), the mean of L.brevicula in the spring, is 56.48; \\(\\beta_{1}\\) is 16.44 thus the mean of L.brevicula in the summer 56.48 + 16.44 = 72.92; \\(\\beta_{2}\\) is 7.28 thus the mean of L.littorea in the spring 56.48 + 7.28 = 72.92; and the L.littorea in the summer is \\(\\beta_{0}+\\beta_{1}+\\beta_{2}+\\beta_{3}\\) = 56.48 \\(+\\) 16.44 \\(+\\) 7.28 \\(+\\) -10.76 \\(=\\) 69.44. Compare to Figure 7.2. 7.6 Getting predictions from the model We already have the predictions for all possible combinations of values of the explanatory variables because both are categorical. However, the code for using predict is included here, as it was in the previous two chapters chapter, because it will make it easier to understand more complex examples later. We need to create a dataframe of values for which we want predictions and pass it as an argument to the predict() function. To create a dataframe with one column of Species values: predict_for &lt;- data.frame(species = rep(c(&quot;Littorina brevicula&quot;, &quot;Littorina littorea&quot;), each = 2), season = rep(c(&quot;Spring&quot;, &quot;Summer&quot;), times = 2)) Remember! The variable and its values have to exactly match those in the model. The to get the predicted myoglobin content for the three species: predict_for$pred &lt;- predict(mod, newdata = predict_for) 7.7 Checking assumptions The two assumptions of the model can be checked using diagnostic plots. The Q-Q plot is obtained with: plot(mod, which = 2) The residual seem to be normally distributed. Let’s look at the Residuals vs Fitted plot: plot(mod, which = 1) The residuals are equally spread around a horizontal line; the assumptions seem to be met. 7.8 Post-hoc testing for lm() Instead of using the TukeyHSD() we will again use the glht() function from the multcomp package (Hothorn, Bretz, and Westfall 2008). A relatively simple way to make all the pairwise comparisons is to create a new factor variable that indicates membership of one of the four groups. We can do that with the interaction() function: periwinkle$seasxspp &lt;- interaction(periwinkle$season, periwinkle$species) para season species seasxspp 58 Spring Littorina brevicula Spring.Littorina brevicula 51 Spring Littorina brevicula Spring.Littorina brevicula 54 Spring Littorina brevicula Spring.Littorina brevicula 39 Spring Littorina brevicula Spring.Littorina brevicula 65 Spring Littorina brevicula Spring.Littorina brevicula 67 Spring Littorina brevicula Spring.Littorina brevicula 60 Spring Littorina brevicula Spring.Littorina brevicula 54 Spring Littorina brevicula Spring.Littorina brevicula 47 Spring Littorina brevicula Spring.Littorina brevicula 66 Spring Littorina brevicula Spring.Littorina brevicula 51 Spring Littorina brevicula Spring.Littorina brevicula 43 Spring Littorina brevicula Spring.Littorina brevicula 62 Spring Littorina brevicula Spring.Littorina brevicula 55 Spring Littorina brevicula Spring.Littorina brevicula 58 Spring Littorina brevicula Spring.Littorina brevicula 43 Spring Littorina brevicula Spring.Littorina brevicula 69 Spring Littorina brevicula Spring.Littorina brevicula 71 Spring Littorina brevicula Spring.Littorina brevicula 64 Spring Littorina brevicula Spring.Littorina brevicula 58 Spring Littorina brevicula Spring.Littorina brevicula 51 Spring Littorina brevicula Spring.Littorina brevicula 70 Spring Littorina brevicula Spring.Littorina brevicula 55 Spring Littorina brevicula Spring.Littorina brevicula 47 Spring Littorina brevicula Spring.Littorina brevicula 54 Spring Littorina brevicula Spring.Littorina brevicula 43 Spring Littorina littorea Spring.Littorina littorea 63 Spring Littorina littorea Spring.Littorina littorea 58 Spring Littorina littorea Spring.Littorina littorea 61 Spring Littorina littorea Spring.Littorina littorea 63 Spring Littorina littorea Spring.Littorina littorea 45 Spring Littorina littorea Spring.Littorina littorea 45 Spring Littorina littorea Spring.Littorina littorea 77 Spring Littorina littorea Spring.Littorina littorea 57 Spring Littorina littorea Spring.Littorina littorea 49 Spring Littorina littorea Spring.Littorina littorea 72 Spring Littorina littorea Spring.Littorina littorea 47 Spring Littorina littorea Spring.Littorina littorea 78 Spring Littorina littorea Spring.Littorina littorea 77 Spring Littorina littorea Spring.Littorina littorea 71 Spring Littorina littorea Spring.Littorina littorea 69 Spring Littorina littorea Spring.Littorina littorea 66 Spring Littorina littorea Spring.Littorina littorea 76 Spring Littorina littorea Spring.Littorina littorea 75 Spring Littorina littorea Spring.Littorina littorea 54 Spring Littorina littorea Spring.Littorina littorea 62 Spring Littorina littorea Spring.Littorina littorea 58 Spring Littorina littorea Spring.Littorina littorea 76 Spring Littorina littorea Spring.Littorina littorea 68 Spring Littorina littorea Spring.Littorina littorea 84 Spring Littorina littorea Spring.Littorina littorea 61 Summer Littorina brevicula Summer.Littorina brevicula 70 Summer Littorina brevicula Summer.Littorina brevicula 68 Summer Littorina brevicula Summer.Littorina brevicula 67 Summer Littorina brevicula Summer.Littorina brevicula 88 Summer Littorina brevicula Summer.Littorina brevicula 64 Summer Littorina brevicula Summer.Littorina brevicula 69 Summer Littorina brevicula Summer.Littorina brevicula 70 Summer Littorina brevicula Summer.Littorina brevicula 56 Summer Littorina brevicula Summer.Littorina brevicula 80 Summer Littorina brevicula Summer.Littorina brevicula 80 Summer Littorina brevicula Summer.Littorina brevicula 53 Summer Littorina brevicula Summer.Littorina brevicula 56 Summer Littorina brevicula Summer.Littorina brevicula 101 Summer Littorina brevicula Summer.Littorina brevicula 81 Summer Littorina brevicula Summer.Littorina brevicula 88 Summer Littorina brevicula Summer.Littorina brevicula 67 Summer Littorina brevicula Summer.Littorina brevicula 73 Summer Littorina brevicula Summer.Littorina brevicula 75 Summer Littorina brevicula Summer.Littorina brevicula 74 Summer Littorina brevicula Summer.Littorina brevicula 68 Summer Littorina brevicula Summer.Littorina brevicula 89 Summer Littorina brevicula Summer.Littorina brevicula 75 Summer Littorina brevicula Summer.Littorina brevicula 74 Summer Littorina brevicula Summer.Littorina brevicula 76 Summer Littorina brevicula Summer.Littorina brevicula 66 Summer Littorina littorea Summer.Littorina littorea 61 Summer Littorina littorea Summer.Littorina littorea 51 Summer Littorina littorea Summer.Littorina littorea 67 Summer Littorina littorea Summer.Littorina littorea 78 Summer Littorina littorea Summer.Littorina littorea 79 Summer Littorina littorea Summer.Littorina littorea 69 Summer Littorina littorea Summer.Littorina littorea 76 Summer Littorina littorea Summer.Littorina littorea 86 Summer Littorina littorea Summer.Littorina littorea 97 Summer Littorina littorea Summer.Littorina littorea 64 Summer Littorina littorea Summer.Littorina littorea 68 Summer Littorina littorea Summer.Littorina littorea 65 Summer Littorina littorea Summer.Littorina littorea 49 Summer Littorina littorea Summer.Littorina littorea 62 Summer Littorina littorea Summer.Littorina littorea 57 Summer Littorina littorea Summer.Littorina littorea 70 Summer Littorina littorea Summer.Littorina littorea 62 Summer Littorina littorea Summer.Littorina littorea 80 Summer Littorina littorea Summer.Littorina littorea 81 Summer Littorina littorea Summer.Littorina littorea 85 Summer Littorina littorea Summer.Littorina littorea 77 Summer Littorina littorea Summer.Littorina littorea 61 Summer Littorina littorea Summer.Littorina littorea 59 Summer Littorina littorea Summer.Littorina littorea 66 Summer Littorina littorea Summer.Littorina littorea We then rebuild the model using this one variable: mod2 &lt;- lm(data = periwinkle, para ~ seasxspp) We have done a one-way ANOVA to obtain our post-hoc comparisons. The parameters in this model will be like those in a one-way ANOVA with \\(\\beta_{3}\\) giving the amount you add to the intercept to get the fourth group mean. Then load the package: library(multcomp) We have to specify our contrasts as a matrix with the linfct (linear functions) argument and there is a multiple comparisons function, mcp(), to help. This is the whole command: mod_mc &lt;- glht(mod2, linfct = mcp(seasxspp = &quot;Tukey&quot;)) You can read this as “do all of the pairwise comparisons between each group in seasxspp in the model mod using the Tukey test”. We view the results with summary(): summary(mod_mc) # # Simultaneous Tests for General Linear Hypotheses # # Multiple Comparisons of Means: Tukey Contrasts # # # Fit: lm(formula = para ~ seasxspp, data = periwinkle) # # Linear Hypotheses: # Estimate # Summer.Littorina brevicula - Spring.Littorina brevicula == 0 16.44 # Spring.Littorina littorea - Spring.Littorina brevicula == 0 7.28 # Summer.Littorina littorea - Spring.Littorina brevicula == 0 12.96 # Spring.Littorina littorea - Summer.Littorina brevicula == 0 -9.16 # Summer.Littorina littorea - Summer.Littorina brevicula == 0 -3.48 # Summer.Littorina littorea - Spring.Littorina littorea == 0 5.68 # Std. Error t value # Summer.Littorina brevicula - Spring.Littorina brevicula == 0 3.09 5.32 # Spring.Littorina littorea - Spring.Littorina brevicula == 0 3.09 2.35 # Summer.Littorina littorea - Spring.Littorina brevicula == 0 3.09 4.19 # Spring.Littorina littorea - Summer.Littorina brevicula == 0 3.09 -2.96 # Summer.Littorina littorea - Summer.Littorina brevicula == 0 3.09 -1.13 # Summer.Littorina littorea - Spring.Littorina littorea == 0 3.09 1.84 # Pr(&gt;|t|) # Summer.Littorina brevicula - Spring.Littorina brevicula == 0 &lt;0.001 *** # Spring.Littorina littorea - Spring.Littorina brevicula == 0 0.093 . # Summer.Littorina littorea - Spring.Littorina brevicula == 0 &lt;0.001 *** # Spring.Littorina littorea - Summer.Littorina brevicula == 0 0.020 * # Summer.Littorina littorea - Summer.Littorina brevicula == 0 0.675 # Summer.Littorina littorea - Spring.Littorina littorea == 0 0.263 # --- # Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # (Adjusted p values reported -- single-step method) The results are the same as for using TukeyHSD() as we have done the same tests using a different function. You can see what a contrasts matrix looks like by looking at the linfct variable of the glht object. You don’t need it now but in the future you may need to specify your own constrasts matrices so let’s have a look to make a step towards understanding: mod_mc$linfct # (Intercept) # Summer.Littorina brevicula - Spring.Littorina brevicula 0 # Spring.Littorina littorea - Spring.Littorina brevicula 0 # Summer.Littorina littorea - Spring.Littorina brevicula 0 # Spring.Littorina littorea - Summer.Littorina brevicula 0 # Summer.Littorina littorea - Summer.Littorina brevicula 0 # Summer.Littorina littorea - Spring.Littorina littorea 0 # seasxsppSummer.Littorina brevicula # Summer.Littorina brevicula - Spring.Littorina brevicula 1 # Spring.Littorina littorea - Spring.Littorina brevicula 0 # Summer.Littorina littorea - Spring.Littorina brevicula 0 # Spring.Littorina littorea - Summer.Littorina brevicula -1 # Summer.Littorina littorea - Summer.Littorina brevicula -1 # Summer.Littorina littorea - Spring.Littorina littorea 0 # seasxsppSpring.Littorina littorea # Summer.Littorina brevicula - Spring.Littorina brevicula 0 # Spring.Littorina littorea - Spring.Littorina brevicula 1 # Summer.Littorina littorea - Spring.Littorina brevicula 0 # Spring.Littorina littorea - Summer.Littorina brevicula 1 # Summer.Littorina littorea - Summer.Littorina brevicula 0 # Summer.Littorina littorea - Spring.Littorina littorea -1 # seasxsppSummer.Littorina littorea # Summer.Littorina brevicula - Spring.Littorina brevicula 0 # Spring.Littorina littorea - Spring.Littorina brevicula 0 # Summer.Littorina littorea - Spring.Littorina brevicula 1 # Spring.Littorina littorea - Summer.Littorina brevicula 0 # Summer.Littorina littorea - Summer.Littorina brevicula 1 # Summer.Littorina littorea - Spring.Littorina littorea 1 # attr(,&quot;type&quot;) # [1] &quot;Tukey&quot; It is matrix with a column for each parameter, in order) and a row for each contrast containing only 0s, 1s and -1s. The rows are named. The numbers are how the model parameters are needed to make the contrast and these can be understood by considering how the group means relate to the parameters. Spring.Littorina brevicula mean is \\(\\beta_{0}\\) Summer.Littorina brevicula mean is \\(\\beta_{0} + \\beta_{1}\\) Spring.Littorina littorea \\(\\beta_{0} + \\beta_{2}\\) Summer.Littorina littorea \\(\\beta_{0} + \\beta_{3}\\) Therefore: * Summer.Littorina brevicula - Spring.Littorina brevicula is: \\(\\beta_{0} + \\beta_{1} - \\beta_{0} = \\beta_{1}\\) and there is a one in the seasxsppSummer.Littorina brevicula column and zeros else where * Spring.Littorina littorea - Spring.Littorina brevicula is: \\(\\beta_{0} + \\beta_{2} - \\beta_{0} = \\beta_{2}\\) and there is a one in the seasxsppSpring.Littorina littorea column and zeros else where * Summer.Littorina littorea - Spring.Littorina brevicula is: \\(\\beta_{0} + \\beta_{3} - \\beta_{0} = \\beta_{3}\\) and there is a 1 in the the seasxsppSummer.Littorina littorea column and zeros else where * Spring.Littorina littorea - Summer.Littorina brevicula is: \\(\\beta_{0} + \\beta_{2} - (\\beta_{0} + \\beta_{1}) = \\beta_{2} - \\beta_{1}\\) and there is a 1 in the the seasxsppSpring.Littorina littorea column and a -1 in the seasxsppSummer.Littorina brevicula column * Summer.Littorina littorea - Summer.Littorina brevicula is: \\(\\beta_{0} + \\beta_{3} - (\\beta_{0} + \\beta_{1}) = \\beta_{3} - \\beta_{1}\\) and there is a 1 in the the seasxsppSummer.Littorina littorea column and a -1 in the seasxsppSummer.Littorina brevicula column * Summer.Littorina littorea - Spring.Littorina littorea is: \\(\\beta_{0} + \\beta_{3} - (\\beta_{0} + \\beta_{2}) = \\beta_{3} + \\beta_{2}\\) and there is a 1 in the the seasxsppSummer.Littorina littorea column and column and a -1 in the seasxsppSpring.Littorina littorea colum 7.9 Creating a figure # palette # blue, pink, green triadic pal4 &lt;- c(&quot;#256c7a&quot;, &quot;#7a256c&quot;, &quot;#6c7a25&quot;) ggplot() + geom_point(data = periwinkle, aes(x = season, y = para, colour = species), position = position_jitterdodge(dodge.width = 1, jitter.width = 0.4, jitter.height = 0), size = 2) + geom_errorbar(data = peri_summary, aes(x = season, ymin = mean - se, ymax = mean + se, group = species), width = 0.4, size = 1, position = position_dodge(width = 1)) + geom_errorbar(data = peri_summary, aes(x = season, ymin = mean, ymax = mean, group = species), width = 0.3, size = 1, position = position_dodge(width = 1) ) + scale_x_discrete(name = &quot;Season&quot;) + scale_y_continuous(name = &quot;Number of parasites&quot;, expand = c(0, 0), limits = c(0, 128)) + scale_colour_manual(values = pal4[1:2], labels = c(bquote(italic(&quot;L.brevicula&quot;)), bquote(italic(&quot;L.littorea&quot;)))) + # Spring:Littorina brevicula-Summer:Littorina littorea * annotate(&quot;segment&quot;, x = 1.25, xend = 1.75, y = 110, yend = 110, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 1.25, xend = 1.25, y = 110, yend = 105, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 1.75, xend = 1.75, y = 110, yend = 105, colour = &quot;black&quot;) + annotate(&quot;text&quot;, x = 1.5, y = 112, label = &quot;***&quot;, size = 6) + # Summer:Littorina brevicula-Spring:Littorina littorea: *** annotate(&quot;segment&quot;, x = 1.25, xend = 0.75, y = 90, yend = 90, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 1.25, xend = 1.25, y = 90, yend = 85, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 0.75, xend = 0.75, y = 90, yend = 85, colour = &quot;black&quot;) + annotate(&quot;text&quot;, x = 1, y = 92, label = &quot;*&quot;, size = 6) + # Summer:Littorina littorea-Spring:Littorina littorea: *** annotate(&quot;segment&quot;, x = 0.75, xend = 1.75, y = 120, yend = 120, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 0.75, xend = 0.75, y = 120, yend = 115, colour = &quot;black&quot;) + annotate(&quot;segment&quot;, x = 1.75, xend = 1.75, y = 120, yend = 115, colour = &quot;black&quot;) + annotate(&quot;text&quot;, x = 1.25, y = 123, label = &quot;***&quot;, size = 6) + theme_classic() + theme(legend.title = element_blank(), legend.position = c(0.85, 0.98)) 7.10 Reporting the results to add: principle, sig, magnitude and direction of effects, test result, figure See figure 7.4. Figure 7.4: periwinkles blah blah References "],
["summary.html", "Chapter 8 Summary", " Chapter 8 Summary what these models have in common: Responses must be independent key points where to go next "],
["references.html", "References", " References "]
]
